\documentclass[12pt, a4paper]{report}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
\author{Kevin O'Brien}
\title{Updating techniques for LME models}

\addcontentsline{toc}{section}{Bibliography}

%----------------------------------------------------------------------------------------%
\newpage

\section{Checking model assumptions}
In classical linear regression, it is important to carry out model diagnostic techniques to determine whether or not the distributional assumptions are satisfied. Model diagnostics are also used to determine the influence of unusual observations. 

\citet{schabenberger} describes the examination of model-data agreement as comprising several elements; residual analysis, goodness of fit, collinearity diagnostics and influence analysis.

Influence is defined as the `ability of a single or multiple data points, through their presence or absence

%-extension to LMEs
Model diagnostic techniques , well established for classical models, have since been adapted for use with linear mixed effects models.Diagnostic techniques for LME models are inevitably more difficult to implement, due to the increased complexity.

\subsection{Residual diagnostics}
For classical linear models, residual diagnostics are typically implemented as a plot of the observed residuals and the predicted values. A visual inspection for the presence of trends inform the analyst on the validity of distributional assumptions, and to detect outliers and influential observations.

In linear mixed effects models, diagnostic techniques may consider `conditional' residuals. A conditional residual is the difference between an observed value $y_{i}$ and the conditional predicted value $\hat{y}_{i} $.

\[ \hat{epsilon}_{i} = y_{i} - \hat{y}_{i} = y_{i} - ( X_{i}\hat{beta} + Z_{i}\hat{b}_{i}) \]

However, using conditional residuals for diagnostics presents difficulties, as they tend to be correlatedand their variances may be different for different subgroups, which can lead to erroneous conclusions.

\subsection{Standardized and studentized residuals}
To alleviate the problem caused by inconstant variance, the residuals are scaled (i.e. divided) by their standard deviations. This results in a `standardized residual'. Because true standard deviations are frequently unknown, one can instead divide a residual by the estimated standard deviation to obtain the `studentized residual. Another possible scaled residual is the `Pearson residual' whereby a residual is divided by the standard deviation of the dependent variable. The Pearson residual can be used when the variability of $\hat{\beta}$ is disregarded in the underlying assumptions.

\subsection{Influence analysis}

Likelihood based estimation methods, such as ML and REML, are sensitive to unusual observations. Influence diagnostics are formal techniques that assess the influence of observations on parameter estimates for $\beta$ and $\theta$. A common technique is to refit the model with an observation or group of observations omitted.

\citet{west} examines a group of methods that examine various aspects of influence diagnostics for LME models.
For overall influence, the most common approaches are the `likelihood distance' and the `restricted likelihood distance'. 

Cook's Distance is a well known diagnostic technique used in classical linear models, extended to LME models. 
For LME models, two formulations exist; a Cook's distance that examines the change in fixed fixed parameter estimates, and another that examines the change in random effects parameter estimates. The outcome of either Cook's distance is a scaled change in either $\beta$ or $\theta$.

\newpage
\section{Updating techniques}
The linear mixed effects model is a useful methodology for fitting a wide range of models. However, linear mixed effects models are known to be sensitive to outliers. \citet{christensen} advises that identification of outliers is necessary before conclusions may be drawn from the fitted model.

Standard statistical packages concentrate on calculating and testing parameter estimates without considering the diagnostics of the model.

The assessment of the effects of perturbations in data, on the outcome of the analysis, is known as statistical influence analysis. Influence analysis examines the robustness of the model.

Influence analysis methodologies have been used extensively in classical linear models, and provided the basis for methodologies for use with LME models.

Computationally inexpensive diagnostics tools have been developed to examine the issue of influence \citep{tewomir}.
Studentized residuals, error contrast matrices and the inverse of the response variance covariance matrix are regular components of these tools.


%------------------------------------------------------------------------------------
\newpage

\citet{cook77} greatly expanded the study of residuals and
influence measures. Cook's key observation was the effects of
deleting each observation in turn could be computed without undue
additional computational expense. Consequently deletion
diagnostics have become an integral part of assessing linear
models.

\subsection{Residuals diagnostics in mixed models}

A residual is the difference between an observed quantity and its estimated or predicted value. In the mixed
model you can distinguish marginal residuals $rm$ and conditional residuals $rc$. A marginal residual is the
difference between the observed data and the estimated (marginal) mean.
\\
\\
Standarized residuals are typically used to detect outliers

\subsection{Deletion methods}
Deleting observations has become a popular and extremely useful approach to studying
sensitivity in statistical problems.

\subsection{Case Deletion Diagnostics}
Case-deletion diagnostics provide a useful tool for identifying influential observations and outliers.


The computation of case deletion diagnostics in the classical model is made simple by the fact that
estimates of $\beta$ and $\sigma^2$, which exclude the ith observation, can be computed without re-fitting the
model. Such update formulas are available in the mixed model only if you assume that the covariance
parameters are not affected by the removal of the observation in question. This is rarely a reasonable
assumption.


\subsection{Cook's distance}
Diagnostic methods for fixed effects are generally analogues of methods used in classical linear models.
Diagnostic methods for variance components are based on `one-step' methods.

\citet{cook86} gives a completely general method for assessing the influence of local departures
from assumptions in statistical models.

For fixed effects parameter estimates in LME models, the Cook's distance can be extended to measure influence on these fixed effects.

\[
\mbox{CD}_{i}(\beta) = \frac{(c_{ii} - r_{ii}) \times t^2_{i}}{r_{ii} \times p}
\]

\noindent For random effect estimates, the Cook's distance is

\[
\mbox{CD}_{i}(b) = g{\prime}_{(i)} (I_{r} + \mbox{var}(\hat{b})D)^{-2}\mbox{var}(\hat{b})g_{(i)}.
\]

\noindent Large values for Cook's distance indicate observations for special attention.

\subsection{Likelihood distance}

\subsection{Cost effection computation of inverse}

\bibliography{transferbib}
\end{document} 